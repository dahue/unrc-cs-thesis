{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26e55dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c872859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "ROOT_PATH = os.environ.get(\"ROOT_PATH\")\n",
    "if not ROOT_PATH:\n",
    "    raise ValueError(\"ROOT_PATH environment variable not set. Please set it in your .env file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0e51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch inference with 5 prompts\n",
      "============================================================\n",
      "Loading model: mlx-community/Llama-3.2-1B-Instruct-4bit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 136031.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Processing prompt 1/5\n",
      "Completed in 8.42s\n",
      "--------------------------------------------------\n",
      "Processing prompt 2/5\n",
      "Completed in 8.09s\n",
      "--------------------------------------------------\n",
      "Processing prompt 3/5\n",
      "Completed in 8.28s\n",
      "--------------------------------------------------\n",
      "Processing prompt 4/5\n",
      "Completed in 5.57s\n",
      "--------------------------------------------------\n",
      "Processing prompt 5/5\n",
      "Completed in 1.25s\n",
      "--------------------------------------------------\n",
      "Results saved to inference_results.sql\n",
      "============================================================\n",
      "BATCH PROCESSING COMPLETE\n",
      "Total prompts: 5\n",
      "Successful: 5\n",
      "Failed: 0\n",
      "Total time: 31.62s\n",
      "Average time per prompt: 6.32s\n",
      "Results saved to inference_results.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "MLX LM Batch Inference Script\n",
    "Processes multiple prompts using mlx-community/Llama-3.2-3B-Instruct-4bit model\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "def load_model(model_path: str = \"mlx-community/Llama-3.2-3B-Instruct-4bit\", adapter_path: str = None):\n",
    "    \"\"\"Load the MLX model and tokenizer, optionally with LoRA adapter\"\"\"\n",
    "    print(f\"Loading model: {model_path}\")\n",
    "    \n",
    "    if adapter_path:\n",
    "        print(f\"Loading with adapter: {adapter_path}\")\n",
    "        model, tokenizer = load(model_path, adapter_path=adapter_path)\n",
    "        print(\"Model and adapter loaded successfully!\")\n",
    "    else:\n",
    "        model, tokenizer = load(model_path)\n",
    "        print(\"Model loaded successfully!\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def process_batch(\n",
    "    prompts: List[str], \n",
    "    model, \n",
    "    tokenizer,\n",
    "    max_tokens: int = 512\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Process a batch of prompts and return results\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"Processing prompt {i+1}/{len(prompts)}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            response = generate(model, tokenizer, prompt=prompt, max_tokens=max_tokens, verbose=False)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            result = {\n",
    "                \"prompt_index\": i,\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": response,\n",
    "                \"generation_time\": end_time - start_time,\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                \"prompt_index\": i,\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": None,\n",
    "                \"generation_time\": 0,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            print(f\"Error processing prompt {i+1}: {e}\")\n",
    "        \n",
    "        results.append(result)\n",
    "        print(f\"Completed in {result['generation_time']:.2f}s\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_results(results: List[Dict[str, Any]], output_file: str = \"inference_results.sql\"):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for query in results:\n",
    "            f.write(query['response'] + \"\\n\")\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "def load_prompts_from_file(file_path: str) -> List[str]:\n",
    "    \"\"\"Load prompts from a text file, JSON file, or JSONL file\"\"\"\n",
    "    prompts = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if 'prompt' in data:\n",
    "                    prompts.append(data['prompt'])\n",
    "                else:\n",
    "                    print(f\"Warning: Line {line_num} missing 'prompt' field, skipping\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Warning: Invalid JSON on line {line_num}, skipping: {e}\")\n",
    "    return prompts\n",
    "\n",
    "def main():\n",
    "    MODEL_PATH = \"mlx-community/Llama-3.2-1B-Instruct-4bit\"\n",
    "    MAX_TOKENS = 512\n",
    "    \n",
    "    prompts = load_prompts_from_file(\"valid.jsonl\")\n",
    "    \n",
    "    print(f\"Starting batch inference with {len(prompts)} prompts\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    model, tokenizer = load_model(MODEL_PATH)\n",
    "    \n",
    "    results = process_batch(\n",
    "        prompts=prompts,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_tokens=MAX_TOKENS\n",
    "    )\n",
    "    \n",
    "    save_results(results)\n",
    "    \n",
    "    successful = sum(1 for r in results if r['status'] == 'success')\n",
    "    failed = len(results) - successful\n",
    "    total_time = sum(r['generation_time'] for r in results)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"BATCH PROCESSING COMPLETE\")\n",
    "    print(f\"Total prompts: {len(prompts)}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Total time: {total_time:.2f}s\")\n",
    "    print(f\"Average time per prompt: {total_time/len(prompts):.2f}s\")\n",
    "    print(\"Results saved to inference_results.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2cc6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unrc-cs-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
