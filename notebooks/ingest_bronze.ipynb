{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bronze ingestion complete using schema.sql\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "# Paths\n",
    "SPIDER_DIR = \"/Users/atissera/Developer/repos/unrc-cs-thesis/tmp/spider_data\"\n",
    "NATSQL_DIR = \"/Users/atissera/Developer/repos/unrc-cs-thesis/tmp/NatSQL/NatSQLv1_6\"\n",
    "OUT_DB = \"/Users/atissera/Developer/repos/unrc-cs-thesis/nl2sql/data/bronze/bronze.sqlite\"\n",
    "SCHEMA_FILE = \"/Users/atissera/Developer/repos/unrc-cs-thesis/nl2sql/data/bronze/schema.sql\"\n",
    "os.makedirs(os.path.dirname(OUT_DB), exist_ok=True)\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Connect to SQLite\n",
    "conn = sqlite3.connect(OUT_DB)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Load schema.sql and apply\n",
    "with open(SCHEMA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    schema_sql = f.read()\n",
    "cursor.executescript(schema_sql)\n",
    "\n",
    "# Insert Spider dataset\n",
    "def load_and_insert_json(file_name, source_label):\n",
    "    data = load_json(os.path.join(SPIDER_DIR, file_name))\n",
    "    for idx, record in enumerate(data):\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO spider_dataset \n",
    "            (id, db_id, source, question, question_toks, query, query_toks, query_toks_no_value, sql_json)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                idx,\n",
    "                record[\"db_id\"],\n",
    "                source_label,\n",
    "                record[\"question\"],\n",
    "                json.dumps(record.get(\"question_toks\", [])),\n",
    "                record[\"query\"],\n",
    "                json.dumps(record.get(\"query_toks\", [])),\n",
    "                json.dumps(record.get(\"query_toks_no_value\", [])),\n",
    "                json.dumps(record.get(\"sql\", {}))\n",
    "            )\n",
    "        )\n",
    "\n",
    "def load_and_insert_natsql(file_name, source_label):\n",
    "    data = load_json(os.path.join(NATSQL_DIR, file_name))\n",
    "    for idx, record in enumerate(data):\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO spider_natsql (id, source, natsql)\n",
    "            VALUES (?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                idx,\n",
    "                source_label,\n",
    "                record[\"NatSQL\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "def load_and_insert_table_schema(schema_path, source_label):\n",
    "    schema_list = load_json(schema_path)\n",
    "    for schema in schema_list:\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO spider_tables \n",
    "            (db_id, source, table_names, table_names_original,\n",
    "             column_names, column_names_original,\n",
    "             column_types, primary_keys, foreign_keys)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                schema[\"db_id\"],\n",
    "                source_label,\n",
    "                json.dumps(schema.get(\"table_names\", [])),\n",
    "                json.dumps(schema.get(\"table_names_original\", [])),\n",
    "                json.dumps(schema.get(\"column_names\", [])),\n",
    "                json.dumps(schema.get(\"column_names_original\", [])),\n",
    "                json.dumps(schema.get(\"column_types\", [])),\n",
    "                json.dumps(schema.get(\"primary_keys\", [])),\n",
    "                json.dumps(schema.get(\"foreign_keys\", [])),\n",
    "            )\n",
    "        )\n",
    "\n",
    "load_and_insert_json(\"train_spider.json\", \"train\")\n",
    "# load_and_insert_json(\"train_others.json\", \"train\")\n",
    "load_and_insert_json(\"dev.json\", \"dev\")\n",
    "load_and_insert_json(\"test.json\", \"test\")\n",
    "\n",
    "load_and_insert_natsql(\"train_spider-natsql.json\", \"train\")\n",
    "load_and_insert_natsql(\"dev-natsql.json\", \"dev\")\n",
    "\n",
    "# Insert Spider schema\n",
    "load_and_insert_table_schema(os.path.join(SPIDER_DIR, \"tables.json\"), \"train_dev\")\n",
    "load_and_insert_table_schema(os.path.join(SPIDER_DIR, \"test_tables.json\"), \"test\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print(\"✅ Bronze ingestion complete using schema.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unrc-cs-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
