{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.jsonl...\n",
      "Writing test.jsonl...\n",
      "Writing valid.jsonl...\n",
      "Dataset split and saved: train (7000), test (2147), valid (1034)\n",
      "\n",
      "Verifying file contents:\n",
      "train.jsonl: 7000 lines\n",
      "test.jsonl: 2147 lines\n",
      "valid.jsonl: 1034 lines\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "ROOT_PATH = \"/Users/atissera/Developer/repos/unrc-cs-thesis\"\n",
    "GOLD_DB = f\"{ROOT_PATH}/database/gold/gold.sqlite\"\n",
    "MODEL_TYPE = 'nl2SQL'\n",
    "\n",
    "def get_schema_ddl(entry):\n",
    "    schema_ddl = json.loads(entry[\"simplified_ddl\"])\n",
    "    formatted_schema_ddl = []\n",
    "    for table in schema_ddl:\n",
    "        formatted_schema_ddl.append(f\"# {table}\")\n",
    "    return \"\\n\".join(formatted_schema_ddl)\n",
    "\n",
    "def get_cell_values_sample(entry, max_samples=3):\n",
    "    db_id = entry[\"db_id\"]\n",
    "    db_path = os.path.join(ROOT_PATH, 'tmp', 'spider_data', 'database', db_id, db_id + '.sqlite')\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "    formatted_tables = []\n",
    "\n",
    "    for table in tables:\n",
    "        cursor.execute(f\"PRAGMA table_info({table});\")\n",
    "        columns = [row[1] for row in cursor.fetchall()]  # row[1] is column name\n",
    "\n",
    "        cursor.execute(f\"SELECT * FROM {table} LIMIT {max_samples};\")\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        col_samples = list(zip(*rows)) if rows else [[] for _ in columns]\n",
    "\n",
    "        col_strs = []\n",
    "        for col, vals in zip(columns, col_samples):\n",
    "            val_list = \", \".join(str(v) for v in vals[:max_samples])\n",
    "            col_strs.append(f\"{col}[{val_list}]\")\n",
    "        formatted = f\"# {table}(\" + \", \".join(col_strs) + \")\"\n",
    "        formatted_tables.append(formatted)\n",
    "\n",
    "    conn.close()\n",
    "    return \"\\n\".join(formatted_tables)\n",
    "\n",
    "def get_foreign_keys(entry):\n",
    "    foreign_keys = json.loads(entry[\"foreign_keys\"])\n",
    "    formatted_foreign_keys = []\n",
    "    for fk in foreign_keys:\n",
    "        formatted_foreign_keys.append(f\"# {fk}\")\n",
    "    return \"\\n\".join(formatted_foreign_keys)\n",
    "\n",
    "def create_prompts(entries, template, template_variables='None'):\n",
    "    prompts = []\n",
    "    for entry in entries:\n",
    "        variables = {\n",
    "            \"schema_ddl\": get_schema_ddl(entry),\n",
    "            \"data_samples\": get_cell_values_sample(entry),\n",
    "            \"foreign_keys\": get_foreign_keys(entry),\n",
    "            \"question\": entry[\"question\"],\n",
    "        }\n",
    "        prompts.append(template.format(**variables))\n",
    "    return prompts\n",
    "\n",
    "def create_completions(entries, completion_column=\"query\"):\n",
    "    completions = []\n",
    "    for entry in entries:\n",
    "        completions.append(entry[completion_column])\n",
    "    return completions\n",
    "\n",
    "def create_dataset(entries, template, template_variables='None', completion_column=\"query\"):\n",
    "    processed_data = []\n",
    "    prompts = create_prompts(entries, template)\n",
    "    completions = create_completions(entries, completion_column)\n",
    "    for prompt, completion in zip(prompts, completions):\n",
    "        processed_data.append({\"prompt\": prompt, \"completion\": completion})\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "conn_gold = sqlite3.connect(GOLD_DB)\n",
    "cursor_gold = conn_gold.cursor()\n",
    "\n",
    "table_columns = cursor_gold.execute(\"PRAGMA table_info(gold_dataset)\").fetchall()\n",
    "columns = [column[1] for column in table_columns]\n",
    "\n",
    "cursor_gold.execute(f\"SELECT {', '.join(columns)} FROM gold_dataset WHERE source = 'train'\")\n",
    "train_rows = cursor_gold.fetchall()\n",
    "train_entries = [dict(zip(columns, row)) for row in train_rows]\n",
    "\n",
    "cursor_gold.execute(f\"SELECT {', '.join(columns)} FROM gold_dataset WHERE source = 'dev'\")\n",
    "valid_rows = cursor_gold.fetchall()\n",
    "valid_entries = [dict(zip(columns, row)) for row in valid_rows]\n",
    "\n",
    "cursor_gold.execute(f\"SELECT {', '.join(columns)} FROM gold_dataset WHERE source = 'test'\")\n",
    "test_rows = cursor_gold.fetchall()\n",
    "test_entries = [dict(zip(columns, row)) for row in test_rows]\n",
    "\n",
    "\n",
    "# Load Prompt Template\n",
    "# nl2SQL\n",
    "with open(os.path.join(ROOT_PATH, \"data\", \"template\", MODEL_TYPE, \"prompt_template.md\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    template = f.read()\n",
    "\n",
    "formatter = string.Formatter()\n",
    "template_variables = [field_name for _, field_name, _, _ in formatter.parse(template) if field_name]\n",
    "\n",
    "\n",
    "train_data = create_dataset(train_entries, template, completion_column=\"query\")\n",
    "valid_data = create_dataset(valid_entries, template, completion_column=\"query\")\n",
    "test_data = create_dataset(test_entries, template, completion_column=\"query\")\n",
    "\n",
    "\n",
    "# Write to JSONL files\n",
    "def write_jsonl(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(\"Writing train.jsonl...\")\n",
    "folder_prefix = f\"{ROOT_PATH}/data/training/{MODEL_TYPE}/\"\n",
    "write_jsonl(train_data, folder_prefix+'train.jsonl')\n",
    "print(\"Writing test.jsonl...\")\n",
    "write_jsonl(test_data, folder_prefix+'test.jsonl')\n",
    "print(\"Writing valid.jsonl...\")\n",
    "write_jsonl(valid_data, folder_prefix+'valid.jsonl')\n",
    "\n",
    "print(f\"Dataset split and saved: train ({len(train_data)}), test ({len(test_data)}), valid ({len(valid_data)})\")\n",
    "\n",
    "# Verify file contents\n",
    "def count_lines(filename):\n",
    "    with open(folder_prefix+filename, 'r') as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "print(\"\\nVerifying file contents:\")\n",
    "print(f\"train.jsonl: {count_lines('train.jsonl')} lines\")\n",
    "print(f\"test.jsonl: {count_lines('test.jsonl')} lines\")\n",
    "print(f\"valid.jsonl: {count_lines('valid.jsonl')} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unrc-cs-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
